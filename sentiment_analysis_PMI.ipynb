{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import unicodedata\n",
    "import sys\n",
    "import operator\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "# from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"twitter_vegan_sent_dataset.csv\",encoding='latin1', usecols = ['twit', 'sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>twit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Creepy Outdoor on the speedway: Alli is now in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The nowhere land - not 100% sick, but definate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Seamonkey86 I am on a healthy eating kick! I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>goood morning sheffield, who the fuck text me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Workin the election, driving round checking on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent                                               twit\n",
       "0     0  Creepy Outdoor on the speedway: Alli is now in...\n",
       "1     0  The nowhere land - not 100% sick, but definate...\n",
       "2     0  @Seamonkey86 I am on a healthy eating kick! I ...\n",
       "3     0  goood morning sheffield, who the fuck text me ...\n",
       "4     0  Workin the election, driving round checking on..."
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['twit'] = [re.sub('@[^\\s]+', '', x) for x in data['twit']]\n",
    "data.head()\n",
    "df_clean = data\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_clean['clean'] = df_clean['twit'].astype('str') \n",
    "df_clean.dtypes\n",
    "\n",
    "df_clean[\"tokens\"] = df_clean[\"clean\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>twit</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Creepy Outdoor on the speedway: Alli is now in...</td>\n",
       "      <td>Creepy Outdoor on the speedway: Alli is now in...</td>\n",
       "      <td>[Creepy, Outdoor, on, the, speedway, Alli, is,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The nowhere land - not 100% sick, but definate...</td>\n",
       "      <td>The nowhere land - not 100% sick, but definate...</td>\n",
       "      <td>[The, nowhere, land, not, 100, sick, but, defi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I am on a healthy eating kick! I could only h...</td>\n",
       "      <td>I am on a healthy eating kick! I could only h...</td>\n",
       "      <td>[I, am, on, a, healthy, eating, kick, I, could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>goood morning sheffield, who the fuck text me ...</td>\n",
       "      <td>goood morning sheffield, who the fuck text me ...</td>\n",
       "      <td>[goood, morning, sheffield, who, the, fuck, te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Workin the election, driving round checking on...</td>\n",
       "      <td>Workin the election, driving round checking on...</td>\n",
       "      <td>[Workin, the, election, driving, round, checki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent                                               twit  \\\n",
       "0     0  Creepy Outdoor on the speedway: Alli is now in...   \n",
       "1     0  The nowhere land - not 100% sick, but definate...   \n",
       "2     0   I am on a healthy eating kick! I could only h...   \n",
       "3     0  goood morning sheffield, who the fuck text me ...   \n",
       "4     0  Workin the election, driving round checking on...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  Creepy Outdoor on the speedway: Alli is now in...   \n",
       "1  The nowhere land - not 100% sick, but definate...   \n",
       "2   I am on a healthy eating kick! I could only h...   \n",
       "3  goood morning sheffield, who the fuck text me ...   \n",
       "4  Workin the election, driving round checking on...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Creepy, Outdoor, on, the, speedway, Alli, is,...  \n",
       "1  [The, nowhere, land, not, 100, sick, but, defi...  \n",
       "2  [I, am, on, a, healthy, eating, kick, I, could...  \n",
       "3  [goood, morning, sheffield, who, the, fuck, te...  \n",
       "4  [Workin, the, election, driving, round, checki...  "
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = open(\"stopwords.txt\").read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang = {\n",
    "\"aint\": \"is not\",\n",
    "\"arent\": \"are not\",\n",
    "\"cant\": \"cannot\",\n",
    "\"cantve\": \"cannot have\",\n",
    "\"cause\": \"because\",\n",
    "\"couldve\": \"could have\",\n",
    "\"couldnt\": \"could not\",\n",
    "\"couldntve\": \"could not have\",\n",
    "\"didnt\": \"did not\",\n",
    "\"doesnt\": \"does not\",\n",
    "\"dont\": \"do not\",\n",
    "\"hadnt\": \"had not\",\n",
    "\"hadntve\": \"had not have\",\n",
    "\"hasnt\": \"has not\",\n",
    "\"havent\": \"have not\",\n",
    "\"hed\": \"he would\",\n",
    "\"hedve\": \"he would have\",\n",
    "\"hell\": \"he will\",\n",
    "\"hellve\": \"he he will have\",\n",
    "\"hes\": \"he is\",\n",
    "\"howd\": \"how did\",\n",
    "\"howdy\": \"how do you\",\n",
    "\"howll\": \"how will\",\n",
    "\"hows\": \"how is\",\n",
    "\"Id\": \"I would\",\n",
    "\"Idve\": \"I would have\",\n",
    "\"Ill\": \"I will\",\n",
    "\"Illve\": \"I will have\",\n",
    "\"Im\": \"I am\",\n",
    "\"Ive\": \"I have\",\n",
    "\"id\": \"i would\",\n",
    "\"idve\": \"i would have\",\n",
    "\"ill\": \"i will\",\n",
    "\"illve\": \"i will have\",\n",
    "\"im\": \"i am\",\n",
    "\"ive\": \"i have\",\n",
    "\"isnt\": \"is not\",\n",
    "\"itd\": \"it would\",\n",
    "\"itdve\": \"it would have\",\n",
    "\"itll\": \"it will\",\n",
    "\"itllve\": \"it will have\",\n",
    "\"its\": \"it is\",\n",
    "\"lets\": \"let us\",\n",
    "\"maam\": \"madam\",\n",
    "\"maynt\": \"may not\",\n",
    "\"mightve\": \"might have\",\n",
    "\"mightnt\": \"might not\",\n",
    "\"mightntve\": \"might not have\",\n",
    "\"mustve\": \"must have\",\n",
    "\"mustnt\": \"must not\",\n",
    "\"mustntve\": \"must not have\",\n",
    "\"neednt\": \"need not\",\n",
    "\"needntve\": \"need not have\",\n",
    "\"oclock\": \"of the clock\",\n",
    "\"oughtnt\": \"ought not\",\n",
    "\"oughtntve\": \"ought not have\",\n",
    "\"shant\": \"shall not\",\n",
    "\"shant\": \"shall not\",\n",
    "\"shantve\": \"shall not have\",\n",
    "\"shed\": \"she would\",\n",
    "\"shedve\": \"she would have\",\n",
    "\"shellve\": \"she will have\",\n",
    "\"shes\": \"she is\",\n",
    "\"shouldve\": \"should have\",\n",
    "\"shouldnt\": \"should not\",\n",
    "\"shouldntve\": \"should not have\",\n",
    "\"sove\": \"so have\",\n",
    "\"thatd\": \"that would\",\n",
    "\"thatdve\": \"that would have\",\n",
    "\"thats\": \"that is\",\n",
    "\"thered\": \"there would\",\n",
    "\"theredve\": \"there would have\",\n",
    "\"theres\": \"there is\",\n",
    "\"theyd\": \"they would\",\n",
    "\"theydve\": \"they would have\",\n",
    "\"theyll\": \"they will\",\n",
    "\"theyllve\": \"they will have\",\n",
    "\"theyre\": \"they are\",\n",
    "\"theyve\": \"they have\",\n",
    "\"tove\": \"to have\",\n",
    "\"wasnt\": \"was not\",\n",
    "\"wed\": \"we would\",\n",
    "\"wedve\": \"we would have\",\n",
    "\"wellve\": \"we will have\",\n",
    "\"weve\": \"we have\",\n",
    "\"werent\": \"were not\",\n",
    "\"whatll\": \"what will\",\n",
    "\"whatllve\": \"what will have\",\n",
    "\"whatre\": \"what are\",\n",
    "\"whats\": \"what is\",\n",
    "\"whatve\": \"what have\",\n",
    "\"whens\": \"when is\",\n",
    "\"whenve\": \"when have\",\n",
    "\"whered\": \"where did\",\n",
    "\"wheres\": \"where is\",\n",
    "\"whereve\": \"where have\",\n",
    "\"wholl\": \"who will\",\n",
    "\"whollve\": \"who will have\",\n",
    "\"whos\": \"who is\",\n",
    "\"whove\": \"who have\",\n",
    "\"whys\": \"why is\",\n",
    "\"whyve\": \"why have\",\n",
    "\"willve\": \"will have\",\n",
    "\"wont\": \"will not\",\n",
    "\"wontve\": \"will not have\",\n",
    "\"wouldve\": \"would have\",\n",
    "\"wouldnt\": \"would not\",\n",
    "\"wouldntve\": \"would not have\",\n",
    "\"yall\": \"you all\",\n",
    "\"yalld\": \"you all would\",\n",
    "\"yalldve\": \"you all would have\",\n",
    "\"yallre\": \"you all are\",\n",
    "\"yallve\": \"you all have\",\n",
    "\"youd\": \"you would\",\n",
    "\"youdve\": \"you would have\",\n",
    "\"youll\": \"you will\",\n",
    "\"youllve\": \"you will have\",\n",
    "\"youre\": \"you are\",\n",
    "\"youve\": \"you have\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_clean['tokens'])):\n",
    "    for j in range(len(df_clean['tokens'][i])):\n",
    "        df_clean['tokens'][i][j] = df_clean['tokens'][i][j].lower()\n",
    "        if df_clean['tokens'][i][j] in slang:\n",
    "            df_clean['tokens'][i][j] = slang[df_clean['tokens'][i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [creepy, outdoor, on, the, speedway, alli, is,...\n",
       "1       [the, nowhere, land, not, 100, sick, but, defi...\n",
       "2       [i, am, on, a, healthy, eating, kick, i, could...\n",
       "3       [goood, morning, sheffield, who, the, fuck, te...\n",
       "4       [workin, the, election, driving, round, checki...\n",
       "                              ...                        \n",
       "1871    [eat, only, vegetables, and, fruit, you, ll, b...\n",
       "1872    [has, shiny, healthy, teeth, i, love, dentist,...\n",
       "1873      [cool, i, m, looking, forward, to, reading, it]\n",
       "1874    [ara, sã, â, que, ets, tot, un, quot, pirata, ...\n",
       "1875    [turns, out, the, people, in, aspen, are, tote...\n",
       "Name: tokens, Length: 1876, dtype: object"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for i in df_clean[\"tokens\"]:\n",
    "    for j in list(i):\n",
    "        if j in stop_words or len(j)<=2 or re.search(r'\\d', j):\n",
    "            i.remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [creepy, outdoor, speedway, alli, portugal, se...\n",
       "1       [nowhere, land, sick, definately, healthy, eit...\n",
       "2          [healthy, eating, kick, could, shetland, pony]\n",
       "3       [goood, morning, sheffield, fuck, text, mornin...\n",
       "4       [workin, election, driving, round, checking, j...\n",
       "                              ...                        \n",
       "1871    [eat, vegetables, fruit, healthy, skinny, brea...\n",
       "1872    [shiny, healthy, teeth, love, dentist, appoint...\n",
       "1873                    [cool, looking, forward, reading]\n",
       "1874    [ara, que, ets, tot, quot, pirata, quot, thepi...\n",
       "1875    [turns, people, aspen, totes, know, veganism, ...\n",
       "Name: tokens, Length: 1876, dtype: object"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[\"tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi, word_freq = build_pmi(df_clean[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frequencies(tweets):\n",
    "    word_freq = {}\n",
    "    for tweet in tweets:\n",
    "        for i in range(len(tweet)):\n",
    "            if (tweet[i] in word_freq):\n",
    "                word_freq[tweet[i]] += 1\n",
    "            else:\n",
    "                word_freq[tweet[i]] = 1\n",
    "    word_mtr = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "    for tweet in tweets:\n",
    "        for i in range(len(tweet)):\n",
    "            for j in range(i):\n",
    "                w1, w2 = sorted((tweet[i], tweet[j]))\n",
    "                if w1 != w2:\n",
    "                    word_mtr[w1][w2] += 1\n",
    "    return (word_freq, word_mtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_probabilities(single_freq, mutual_freq, num_tweets):\n",
    "    p_word = {}\n",
    "    p_mutual = defaultdict(lambda : defaultdict(int))\n",
    "    for term1, n in single_freq.items():\n",
    "        p_word[term1] = n / num_tweets\n",
    "        for term2 in mutual_freq[term1]:\n",
    "            p_mutual[term1][term2] = mutual_freq[term1][term2] / num_tweets\n",
    "    return (p_word, p_mutual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pmi(tweets):\n",
    "    word_freq, word_mtr = build_frequencies(tweets)\n",
    "    p_word, p_mutual = build_probabilities(word_freq, word_mtr, len(tweets))\n",
    "    pmi = defaultdict(lambda : defaultdict(int))\n",
    "    \n",
    "    for term1 in p_word:\n",
    "        for term2 in word_mtr[term1]:\n",
    "            denom = p_word[term1] * p_word[term2]\n",
    "            if (denom < 1e-9):\n",
    "                pmi[term1][term2] = 0.0\n",
    "            else:\n",
    "                pmi[term1][term2] = math.log2(p_mutual[term1][term2] / denom)\n",
    "    return pmi, word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = open(\"positive-words.txt\", \"r\", encoding='latin1').read().split()\n",
    "negative_words = open(\"negative-words.txt\", \"r\", encoding='latin1').read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_association(word):\n",
    "    if (word not in word_freq):\n",
    "        # print(\"That word has never been used in this set of tweets! Try another.\")\n",
    "        return 0\n",
    "    else:\n",
    "        return (sum(pmi[word][x] for x in positive_words) - sum(pmi[word][x] for x in negative_words)) / word_freq[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data set\n",
    "pos_words = {}\n",
    "neg_words = {}\n",
    "for j in range(1876):\n",
    "    for i in df_clean[\"tokens\"][j]:\n",
    "        value = get_association(i)\n",
    "        if value > 0:\n",
    "            pos_words.setdefault(i, []).append(value)\n",
    "        elif value < 0:\n",
    "            neg_words.setdefault(i, []).append(value)\n",
    "   # print(j, end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy on a training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6149333333333333\n"
     ]
    }
   ],
   "source": [
    "cnt=0; \n",
    "total = 0;\n",
    "for j in range(len(df_clean)):\n",
    "    if(len(df_clean[\"tokens\"][j]) == 0):\n",
    "        continue;\n",
    "    score = 0\n",
    "    total+=1\n",
    "    for i in df_clean[\"tokens\"][j]:\n",
    "        temp = get_association(i)\n",
    "        score += temp\n",
    "        # print(temp, len(test_data[\"ntweet\"][j]), i)\n",
    "       \n",
    "    score = score / len(df_clean[\"tokens\"][j])\n",
    "    if(df_clean['sent'][j]==0 and score<0):  cnt+=1\n",
    "    elif(df_clean['sent'][j]==4 and score>=0): cnt+=1\n",
    "    \n",
    "    # if score >= 0:\n",
    "    #     print(\"positive\", score, df_clean['sent'][j], cnt/(total), j)\n",
    "    # else: print(\"negative\", score,df_clean['sent'][j], cnt/(total),j)\n",
    "print(cnt/(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos, neg from main data set 2010-2022\n",
    "pos_wordsV = {}\n",
    "neg_wordsV = {}\n",
    "for j in range(len(vegan)):\n",
    "    for i in vegan[\"tokens\"][j]:\n",
    "        value = get_association(i)\n",
    "        if value > 0:\n",
    "            pos_wordsV.setdefault(i, []).append(value)\n",
    "        elif value < 0:\n",
    "            neg_wordsV.setdefault(i, []).append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import unicodedata\n",
    "import sys\n",
    "import operator\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2010-2022 popular tweets analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>quoteCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.136770e+05</td>\n",
       "      <td>113677.000000</td>\n",
       "      <td>113677.000000</td>\n",
       "      <td>113677.000000</td>\n",
       "      <td>113677.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.396452e+17</td>\n",
       "      <td>0.328853</td>\n",
       "      <td>0.528603</td>\n",
       "      <td>3.144655</td>\n",
       "      <td>0.048154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.674333e+17</td>\n",
       "      <td>3.921993</td>\n",
       "      <td>23.023366</td>\n",
       "      <td>344.353841</td>\n",
       "      <td>4.196721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.313936e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.290187e+17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.483031e+17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.145107e+18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.527700e+18</td>\n",
       "      <td>1119.000000</td>\n",
       "      <td>7492.000000</td>\n",
       "      <td>115499.000000</td>\n",
       "      <td>1398.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Tweet Id     replyCount   retweetCount      likeCount  \\\n",
       "count  1.136770e+05  113677.000000  113677.000000  113677.000000   \n",
       "mean   7.396452e+17       0.328853       0.528603       3.144655   \n",
       "std    4.674333e+17       3.921993      23.023366     344.353841   \n",
       "min    7.313936e+09       0.000000       0.000000       0.000000   \n",
       "25%    3.290187e+17       0.000000       0.000000       0.000000   \n",
       "50%    7.483031e+17       0.000000       0.000000       0.000000   \n",
       "75%    1.145107e+18       0.000000       0.000000       1.000000   \n",
       "max    1.527700e+18    1119.000000    7492.000000  115499.000000   \n",
       "\n",
       "          quoteCount  \n",
       "count  113677.000000  \n",
       "mean        0.048154  \n",
       "std         4.196721  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max      1398.000000  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.read_pickle(\"vegan_tweets2010-2022_monthly.pkl\")\n",
    "data2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[\"year\"] = data2[\"Datetime\"].apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Username</th>\n",
       "      <th>Query</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>lang</th>\n",
       "      <th>source</th>\n",
       "      <th>retweetedTweet</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>mentionedUsers</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-30 23:59:47+00:00</td>\n",
       "      <td>8430272628</td>\n",
       "      <td>Well help me understand how that relates to b...</td>\n",
       "      <td>IQXS</td>\n",
       "      <td>vegan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/Twinz2]</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-30 23:58:21+00:00</td>\n",
       "      <td>8430226196</td>\n",
       "      <td>okay, maybe i'm not making vegan alfredo saude...</td>\n",
       "      <td>desdemona</td>\n",
       "      <td>vegan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-30 23:57:14+00:00</td>\n",
       "      <td>8430191199</td>\n",
       "      <td>Via  no oil #vegan pesto ~ easy-shmeasy http:/...</td>\n",
       "      <td>itsallgood3sm</td>\n",
       "      <td>vegan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://www.cloudhopper.com/\" rel=\"nof...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/feelgoodguru]</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-30 23:56:25+00:00</td>\n",
       "      <td>8430165062</td>\n",
       "      <td>Just had a delicious vegan meal @ Jean's Kitch...</td>\n",
       "      <td>RachelLagroix</td>\n",
       "      <td>vegan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-30 23:56:19+00:00</td>\n",
       "      <td>8430161604</td>\n",
       "      <td>says: I like how this BBQ sauce is vegan. Wha...</td>\n",
       "      <td>annesmash</td>\n",
       "      <td>vegan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[https://twitter.com/secretKGB]</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime    Tweet Id  \\\n",
       "0 2010-01-30 23:59:47+00:00  8430272628   \n",
       "1 2010-01-30 23:58:21+00:00  8430226196   \n",
       "2 2010-01-30 23:57:14+00:00  8430191199   \n",
       "3 2010-01-30 23:56:25+00:00  8430165062   \n",
       "4 2010-01-30 23:56:19+00:00  8430161604   \n",
       "\n",
       "                                                Text       Username  Query  \\\n",
       "0   Well help me understand how that relates to b...           IQXS  vegan   \n",
       "1  okay, maybe i'm not making vegan alfredo saude...      desdemona  vegan   \n",
       "2  Via  no oil #vegan pesto ~ easy-shmeasy http:/...  itsallgood3sm  vegan   \n",
       "3  Just had a delicious vegan meal @ Jean's Kitch...  RachelLagroix  vegan   \n",
       "4   says: I like how this BBQ sauce is vegan. Wha...      annesmash  vegan   \n",
       "\n",
       "   replyCount  retweetCount  likeCount  quoteCount lang  \\\n",
       "0           0             0          0           0   en   \n",
       "1           0             0          0           0   en   \n",
       "2           0             0          0           0   en   \n",
       "3           0             0          0           0   en   \n",
       "4           0             0          0           0   en   \n",
       "\n",
       "                                              source retweetedTweet  \\\n",
       "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...           None   \n",
       "1  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...           None   \n",
       "2  <a href=\"http://www.cloudhopper.com/\" rel=\"nof...           None   \n",
       "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...           None   \n",
       "4  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...           None   \n",
       "\n",
       "  quotedTweet                      mentionedUsers  year  \n",
       "0        None        [https://twitter.com/Twinz2]  2010  \n",
       "1        None                                None  2010  \n",
       "2        None  [https://twitter.com/feelgoodguru]  2010  \n",
       "3        None                                None  2010  \n",
       "4        None     [https://twitter.com/secretKGB]  2010  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['Text'] = [re.sub('@[^\\s]+', '', x) for x in data2['Text']]\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegan = data2\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "vegan['clean'] = vegan['Text'].astype('str') \n",
    "vegan.dtypes\n",
    "\n",
    "vegan[\"tokens\"] = vegan[\"Text\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [Well, help, me, understand, how, that, relate...\n",
       "1         [okay, maybe, i, m, not, making, vegan, alfred...\n",
       "2         [Via, no, oil, vegan, pesto, easy, shmeasy, ht...\n",
       "3         [Just, had, a, delicious, vegan, meal, Jean, s...\n",
       "4         [says, I, like, how, this, BBQ, sauce, is, veg...\n",
       "                                ...                        \n",
       "113672                                              [༥, ｽﾔ]\n",
       "113673                                     [ついうっかり, お疲れ様です]\n",
       "113674                                         [お前誰だよ, キモい]\n",
       "113675    [Wie, werden, die, aus, Tofu, hergestellt, Gem...\n",
       "113676    [My, sister, will, have, her, battered, tofu, ...\n",
       "Name: tokens, Length: 113677, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vegan['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vegan)):\n",
    "    for j in range(len(vegan['tokens'][i])):\n",
    "        vegan['tokens'][i][j] = vegan['tokens'][i][j].lower()\n",
    "        if vegan['tokens'][i][j] in slang:\n",
    "            vegan['tokens'][i][j] = slang[vegan['tokens'][i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vegan[\"tokens\"]:\n",
    "    for j in list(i):\n",
    "        if j in stop_words or len(j)<=2 or re.search(r'\\d', j):\n",
    "            i.remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, help, understand, relates, vegan, serious]\n",
       "1    [okay, maybe, making, vegan, alfredo, saude, n...\n",
       "2    [oil, vegan, pesto, easy, shmeasy, http, feelg...\n",
       "3    [delicious, vegan, meal, jean, kitchen, danforth]\n",
       "4    [says, like, bbq, sauce, vegan, going, bbq, fr...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vegan[\"tokens\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of popular tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE 2010     0.4830917874396135           200.0\n",
      "NEGATIVE 2010     0.34299516908212563           142.0\n",
      "NEUTRAL 2010     0.17391304347826086           72.0\n",
      "414\n",
      "POSITIVE 2011     0.4833948339483395           262.0\n",
      "NEGATIVE 2011     0.3007380073800738           163.0\n",
      "NEUTRAL 2011     0.2158671586715867           117.0\n",
      "542\n",
      "POSITIVE 2012     0.43221476510067114           322.0\n",
      "NEGATIVE 2012     0.3167785234899329           236.0\n",
      "NEUTRAL 2012     0.25100671140939596           187.0\n",
      "745\n",
      "POSITIVE 2013     0.4778856526429342           443.0\n",
      "NEGATIVE 2013     0.313915857605178           291.0\n",
      "NEUTRAL 2013     0.20819848975188782           193.0\n",
      "927\n",
      "POSITIVE 2014     0.3960474308300395           501.0\n",
      "NEGATIVE 2014     0.30355731225296445           384.0\n",
      "NEUTRAL 2014     0.30039525691699603           380.0\n",
      "1265\n",
      "POSITIVE 2015     0.42025518341307816           527.0\n",
      "NEGATIVE 2015     0.2902711323763955           364.0\n",
      "NEUTRAL 2015     0.2894736842105263           363.0\n",
      "1254\n",
      "POSITIVE 2016     0.4641589779985806           654.0\n",
      "NEGATIVE 2016     0.26969481902058196           380.0\n",
      "NEUTRAL 2016     0.26614620298083747           375.0\n",
      "1409\n",
      "POSITIVE 2017     0.4659985683607731           651.0\n",
      "NEGATIVE 2017     0.2949176807444524           412.0\n",
      "NEUTRAL 2017     0.23908375089477452           334.0\n",
      "1397\n",
      "POSITIVE 2018     0.4677804295942721           588.0\n",
      "NEGATIVE 2018     0.2911694510739857           366.0\n",
      "NEUTRAL 2018     0.24105011933174225           303.0\n",
      "1257\n",
      "POSITIVE 2019     0.457000710732054           643.0\n",
      "NEGATIVE 2019     0.3191186922530206           449.0\n",
      "NEUTRAL 2019     0.22388059701492538           315.0\n",
      "1407\n",
      "POSITIVE 2020     0.45570630486831604           571.0\n",
      "NEGATIVE 2020     0.3120510774142059           391.0\n",
      "NEUTRAL 2020     0.23224261771747806           291.0\n",
      "1253\n",
      "POSITIVE 2021     0.5012224938875306           615.0\n",
      "NEGATIVE 2021     0.31703341483292585           389.0\n",
      "NEUTRAL 2021     0.1817440912795436           223.0\n",
      "1227\n",
      "POSITIVE 2022 0.453125 203.0\n",
      "NEGATIVE 2022 0.29464285714285715 132.0\n",
      "NEUTRAL 2022     0.25223214285714285           113.0\n",
      "448\n"
     ]
    }
   ],
   "source": [
    "cnt=0;\n",
    "pos = 0.0\n",
    "neg =0.0\n",
    "nt = 0.0\n",
    "ye = 2010;\n",
    "\n",
    "for j in range(len(vegan)):\n",
    "    #ignore tweets with no retweets\n",
    "    if vegan['retweetCount'][j] < 1: continue \n",
    "    if ye != vegan['year'][j]:\n",
    "        if cnt != 0:\n",
    "            print(\"POSITIVE \"+ str(ye) + \"     \" + str(pos/cnt) + '           ' + str(pos))\n",
    "            print(\"NEGATIVE \"+ str(ye) + \"     \" + str(neg/cnt) + '           ' + str(neg))\n",
    "            print(\"NEUTRAL \"+ str(ye) + \"     \" + str(nt/cnt) + '           ' + str(nt))\n",
    "            print(cnt)\n",
    "        cnt = 0\n",
    "        pos = 0.0\n",
    "        neg = 0.0\n",
    "        nt = 0.0\n",
    "        ye +=1\n",
    "    score = 0\n",
    "    for i in vegan[\"tokens\"][j]:\n",
    "        temp = get_association(i)\n",
    "        score += temp\n",
    "    if len(vegan[\"tokens\"][j]) == 0:\n",
    "        continue;\n",
    "    score = score / len(vegan[\"tokens\"][j])\n",
    "    cnt +=1\n",
    "    if(score >=-0.1 and score <=0.1): \n",
    "        nt +=1\n",
    "       \n",
    "    elif score >= 0:\n",
    "        pos +=1\n",
    "        \n",
    "    else: \n",
    "        neg+=1\n",
    "        \n",
    "    #print(str(score) + ' ' +str(vegan['Text'][j]))\n",
    "   \n",
    "    #for diffusion\n",
    "    if (vegan['retweetCount'][j] >=200):\n",
    "        #print(str(score) + '  retweet' +str(vegan['retweetCount'][j]))\n",
    "        # print(str(score) + ' ' +str(vegan['Text'][j]))\n",
    "        x = np.append(x, score)\n",
    "        y = np.append(y,vegan['retweetCount'][j] )\n",
    "        \n",
    "print(\"POSITIVE \"+ str(ye) + \" \" + str(pos/cnt) + \" \" +str(pos))       \n",
    "print(\"NEGATIVE \"+ str(ye) + \" \" + str(neg/cnt) +\" \"+ str(neg))\n",
    "print(\"NEUTRAL \"+ str(ye) + \"     \" + str(nt/cnt) + '           ' + str(nt))\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here I sort x values and y values\n",
    "#here we count only tweets having more than 200 retweets \n",
    "args = np.argsort(x)\n",
    "x = x[args]\n",
    "y = y[args]\n",
    "\n",
    "plt.scatter(x, y) \n",
    "\n",
    "fit = np.polyfit(x, y, deg=4) \n",
    "p = np.poly1d(fit) \n",
    "plt.plot(x,p(x),\"r--\") \n",
    "plt.xlabel(\"Sentiment value\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "x=np.array([])\n",
    "y = np.array([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
